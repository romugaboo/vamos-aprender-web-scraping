# ğŸ’» Vamos Aprender Web Scraping!

Este repositÃ³rio tem como objetivo ensinar o bÃ¡sico de **web scraping**, focando no uso da biblioteca **Selenium** com Python para coletar dados de sites com conteÃºdo dinÃ¢mico.

## ğŸ“š O que Ã© Web Scraping?

**Web scraping** (ou raspagem de dados) Ã© o processo de extrair informaÃ§Ãµes automaticamente de sites. Ele permite coletar grandes volumes de dados de forma estruturada e reutilizÃ¡vel.

## ğŸ”§ Ferramentas de Web Scraping

As principais abordagens sÃ£o:

- ğŸ”— [**JSON direto da API**](https://developer.mozilla.org/pt-BR/docs/Web/HTTP/Overview): ideal quando a pÃ¡gina consome dados de uma API. Basta interceptar e coletar o link.
- ğŸ”— [**BeautifulSoup**](https://www.crummy.com/software/BeautifulSoup/bs4/doc/): ideal quando os dados estÃ£o no HTML estÃ¡tico e nÃ£o hÃ¡ necessidade de interaÃ§Ã£o (cliques, rolagens).
- ğŸ”— [**Selenium**](https://www.selenium.dev/documentation/webdriver/): necessÃ¡ria quando o conteÃºdo Ã© carregado dinamicamente via JavaScript (AJAX) ou hÃ¡ interaÃ§Ãµes na pÃ¡gina.

**Regra prÃ¡tica:**

- Se nÃ£o precisar clicar: **use BeautifulSoup** (mais rÃ¡pido e estÃ¡vel).
- Se precisar interagir com a pÃ¡gina: **use Selenium**.

## ğŸš€ Por que aprender Selenium?

Apesar de mais pesado e lento, **Selenium** Ã© o mais versÃ¡til. Ele permite:

- Simular cliques, rolagens e interaÃ§Ãµes humanas
- Coletar dados dinÃ¢micos (ex: AJAX)
- Testes automatizados end-to-end (E2E), inclusive em ambientes de produÃ§Ã£o (como AWS)

Saber usar o Selenium cobre praticamente todos os cenÃ¡rios. A transiÃ§Ã£o para BeautifulSoup depois Ã© simples.

## ğŸ§ª Uso intenso do DevTools

Durante o desenvolvimento, usamos frequentemente o **DevTools (F12)** do navegador, especialmente a aba **Elements** para inspecionar a estrutura do HTML. Isso ajuda a:

- Encontrar o seletor correto dos elementos (como `div.product`, `span.price`, etc)
- Copiar XPaths ou seletores CSS rapidamente

Saber navegar bem no DevTools Ã© essencial para scraping eficiente.

## ğŸŒ Navegador usado

Apesar da preferÃªncia pessoal pelo **Firefox**, o projeto foi feito usando **Google Chrome** porque:

- O Selenium tem integraÃ§Ã£o mais estÃ¡vel com o **ChromeDriver**
- O Firefox com GeckoDriver impÃµe limites de uso e pode apresentar travamentos ou incompatibilidades com alguns sites modernos
- A maioria dos exemplos e documentaÃ§Ã£o da comunidade estÃ¡ voltada para o Chrome

VocÃª pode adaptar o cÃ³digo para Firefox se quiser, mas o Chrome Ã© mais confiÃ¡vel para scraping automatizado.

## ğŸŒ Site para prÃ¡tica

Vamos praticar com o seguinte site:

- ğŸ”¸ **AJAX (com cliques necessÃ¡rios)**  
  https://webscraper.io/test-sites/e-commerce/ajax  
  Requer Selenium pois o conteÃºdo dos produtos sÃ³ aparece apÃ³s o clique.

- ğŸ”¹ **Site estÃ¡tico (sem necessidade de clique)**  
  https://webscraper.io/test-sites/e-commerce/static  
  Ideal para BeautifulSoup, pois o HTML Ã© carregado diretamente.

## ğŸ§  EstratÃ©gia da Aula

- Vamos **extrair dados** de todas as categorias e subcategorias do site com AJAX.
- Os dados extraÃ­dos serÃ£o:

  - Categoria
  - Subcategoria
  - Nome do produto
  - PreÃ§o
  - DescriÃ§Ã£o
  - NÃºmero de reviews
  - AvaliaÃ§Ã£o (estrelas)

- Todos os dados serÃ£o salvos em um arquivo `.csv`.

Durante a execuÃ§Ã£o, exibiremos o progresso no terminal para verificar se o AJAX carregou tudo corretamente.

**Importante:**  
Vamos focar apenas na **extraÃ§Ã£o dos dados**. A **limpeza e anÃ¡lise** serÃ£o feitas separadamente.

## ğŸ’¾ Por que salvar sÃ³ os dados?

Separar scraping da anÃ¡lise Ã© **boa prÃ¡tica**:

- Garante que vocÃª salve os dados mesmo se o site sair do ar ou a internet cair
- A coleta fica mais rÃ¡pida e focada
- VocÃª pode trabalhar offline com os dados depois

## âš™ï¸ Requisitos para rodar o projeto

- Python 3.x ou superior
- Google Chrome instalado

Instale as dependÃªncias com:

```bash
pip install selenium
```

## ğŸ§© ExtensÃµes do VSCode Ãºteis

- **[Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python)**
  Suporte completo Ã  linguagem Python no VSCode. Permite debug, linting, autocomplete, execuÃ§Ã£o e integraÃ§Ã£o com ambientes virtuais.

- **[Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner)**
  Executa scripts Python com um clique ou atalho (`Ctrl+Alt+N`). Ideal para testar rapidamente funÃ§Ãµes ou scripts completos sem configurar o terminal.

- **[Rainbow CSV](https://marketplace.visualstudio.com/items?itemName=mechatroner.rainbow-csv)**
  Destaca automaticamente colunas de arquivos `.csv` com cores diferentes, facilitando a leitura e debug de dados tabulares diretamente no VSCode.

- **[Markdown Viewer Enhanced](https://marketplace.visualstudio.com/items?itemName=MarkdownViewer.enhanced-md-editor)**
  Visualiza arquivos `.md` com renderizaÃ§Ã£o real, facilitando a ediÃ§Ã£o de READMEs e documentaÃ§Ã£o diretamente dentro do editor.

- **[Windsurf](https://marketplace.visualstudio.com/items?itemName=Codeium.codeium)**
  Ferramenta de autocompletar com IA. Sugere trechos de cÃ³digo inteiros em tempo real, com base no contexto do que vocÃª estÃ¡ escrevendo.

## ğŸ“ AnÃ¡lise Posterior

ApÃ³s a coleta, vocÃª pode tratar, analisar e visualizar os dados com:

- ğŸ“Š [**Pandas**](https://pandas.pydata.org/docs/): carregar o CSV em um DataFrame, filtrar, agrupar e calcular mÃ©tricas.
- ğŸ“ˆ [**Matplotlib**](https://matplotlib.org/stable/index.html) / [**Seaborn**](https://seaborn.pydata.org/): gerar grÃ¡ficos visuais.
- ğŸ“‹ **Excel**: nem sempre a formataÃ§Ã£o fica correta ao abrir o CSV diretamente no excel. Se quiser algo mais visual, abra uma planilha em branco, vÃ¡ para a aba dados, clique em "De Text \ CSV", escolha o arquivo a ser convertido e salve como `.xlsx`.

## ğŸ§© Desafio (Extra)

- âœ… Utilize um bloco `try/except` para garantir que os dados coletados atÃ© o momento sejam salvos mesmo se algo der errado (ex: internet caiu, exceÃ§Ã£o inesperada, luz acabou).
- âœ… Pegue as mesmas informaÃ§Ãµes utilizando uma tÃ©cnica que nÃ£o foi ensinada no vÃ­deo (leia a documentaÃ§Ã£o oficial do selenium).
- âœ… Clique em cada produto individualmente e extraia a capacidade do HD e a cor (caso existam essas informaÃ§Ãµes na pÃ¡gina do produto).
 
 